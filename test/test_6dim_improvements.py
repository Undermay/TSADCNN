#!/usr/bin/env python3
"""æµ‹è¯•6ç»´æ¨¡å‹ç›¸æ¯”4ç»´æ¨¡å‹çš„æ”¹è¿›æ•ˆæœ"""

import sys
sys.path.append('.')
from train_simplified import create_model
import yaml
import torch
import numpy as np
from utils.contrastive_data_loader import create_contrastive_data_loaders

def test_model_improvements():
    """æµ‹è¯•æ¨¡å‹æ”¹è¿›æ•ˆæœ"""
    print("=== æµ‹è¯•6ç»´æ¨¡å‹æ”¹è¿›æ•ˆæœ ===")
    
    # åŠ è½½é…ç½®
    with open('config_improved_v4.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print(f'âœ“ é…ç½®ä¸­çš„input_dim: {config["model"]["input_dim"]}')
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)
    print(f'âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, test_loader = create_contrastive_data_loaders(
        train_path='data/csv/train_correct.csv',
        test_path='data/csv/test_correct.csv',
        batch_size=32,
        num_workers=0,
        normalize=True,
        use_minmax_normalization=True,
        normalization_mode='segment',
        group_by_scene=True
    )
    
    print(f'âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ')
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    model.eval()
    with torch.no_grad():
        batch = next(iter(test_loader))
        old_traj, new_traj, labels = batch
        
        print(f'âœ“ æµ‹è¯•æ‰¹æ¬¡å½¢çŠ¶: old_traj={old_traj.shape}, new_traj={new_traj.shape}')
        
        # å‰å‘ä¼ æ’­
        old_emb, new_emb, loss_dict = model(old_traj, new_traj, labels)
        
        print(f'âœ“ åµŒå…¥å‘é‡å½¢çŠ¶: old_emb={old_emb.shape}, new_emb={new_emb.shape}')
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        old_emb_norm = torch.nn.functional.normalize(old_emb, p=2, dim=1)
        new_emb_norm = torch.nn.functional.normalize(new_emb, p=2, dim=1)
        cosine_sim = torch.sum(old_emb_norm * new_emb_norm, dim=1)
        
        # åˆ†ææ­£è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ
        positive_mask = labels == 1
        negative_mask = labels == 0
        
        pos_similarities = cosine_sim[positive_mask]
        neg_similarities = cosine_sim[negative_mask]
        
        print(f'\n=== ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ ===')
        print(f'æ­£æ ·æœ¬ç›¸ä¼¼åº¦: å‡å€¼={pos_similarities.mean():.4f}, æ ‡å‡†å·®={pos_similarities.std():.4f}')
        print(f'è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦: å‡å€¼={neg_similarities.mean():.4f}, æ ‡å‡†å·®={neg_similarities.std():.4f}')
        print(f'æ­£è´Ÿæ ·æœ¬åˆ†ç¦»åº¦: {pos_similarities.mean() - neg_similarities.mean():.4f}')
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¼‚å¸¸é«˜çš„ç›¸ä¼¼åº¦
        high_sim_threshold = 0.95
        high_sim_count = (cosine_sim > high_sim_threshold).sum().item()
        print(f'å¼‚å¸¸é«˜ç›¸ä¼¼åº¦(>{high_sim_threshold})æ ·æœ¬æ•°: {high_sim_count}/{len(cosine_sim)}')
        
        # åˆ†æåµŒå…¥å‘é‡çš„å¤šæ ·æ€§
        embedding_std = old_emb.std(dim=0).mean().item()
        print(f'åµŒå…¥å‘é‡å¤šæ ·æ€§(æ ‡å‡†å·®): {embedding_std:.4f}')
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨NaN
        has_nan = torch.isnan(cosine_sim).any().item()
        print(f'æ˜¯å¦å­˜åœ¨NaN: {"æ˜¯" if has_nan else "å¦"}')
        
        print(f'\n=== æŸå¤±ä¿¡æ¯ ===')
        if isinstance(loss_dict, dict):
            for key, value in loss_dict.items():
                if hasattr(value, 'item'):
                    print(f'{key}: {value.item():.4f}')
        
        # è¯„ä¼°æ”¹è¿›æ•ˆæœ
        print(f'\n=== æ”¹è¿›æ•ˆæœè¯„ä¼° ===')
        
        # 1. ç»´åº¦åŒ¹é…
        expected_dim = 6
        actual_dim = old_traj.shape[-1]
        print(f'âœ“ ç»´åº¦åŒ¹é…: æœŸæœ›{expected_dim}ç»´, å®é™…{actual_dim}ç»´ - {"é€šè¿‡" if actual_dim == expected_dim else "å¤±è´¥"}')
        
        # 2. æ•°å€¼ç¨³å®šæ€§
        print(f'âœ“ æ•°å€¼ç¨³å®šæ€§: {"é€šè¿‡" if not has_nan else "å¤±è´¥"}')
        
        # 3. ç‰¹å¾å¤šæ ·æ€§
        diversity_threshold = 0.1
        print(f'âœ“ ç‰¹å¾å¤šæ ·æ€§: {"é€šè¿‡" if embedding_std > diversity_threshold else "éœ€è¦æ”¹è¿›"}')
        
        # 4. æ­£è´Ÿæ ·æœ¬åˆ†ç¦»
        separation = pos_similarities.mean() - neg_similarities.mean()
        separation_threshold = 0.1
        print(f'âœ“ æ­£è´Ÿæ ·æœ¬åˆ†ç¦»: {"é€šè¿‡" if separation > separation_threshold else "éœ€è¦æ”¹è¿›"}')
        
        return {
            'dimension_match': actual_dim == expected_dim,
            'numerical_stability': not has_nan,
            'feature_diversity': embedding_std > diversity_threshold,
            'sample_separation': separation > separation_threshold,
            'pos_sim_mean': pos_similarities.mean().item(),
            'neg_sim_mean': neg_similarities.mean().item(),
            'separation': separation,
            'embedding_std': embedding_std
        }

if __name__ == "__main__":
    try:
        results = test_model_improvements()
        
        print(f'\nğŸ¯ æ€»ä½“è¯„ä¼°ç»“æœ:')
        all_passed = all([
            results['dimension_match'],
            results['numerical_stability'], 
            results['feature_diversity'],
            results['sample_separation']
        ])
        
        if all_passed:
            print('ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼6ç»´æ¨¡å‹å·¥ä½œæ­£å¸¸ã€‚')
        else:
            print('âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚')
            
    except Exception as e:
        print(f'âŒ æµ‹è¯•å¤±è´¥: {e}')
        import traceback
        traceback.print_exc()