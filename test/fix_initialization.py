#!/usr/bin/env python3
"""
ä¿®å¤ä½™å¼¦ç›¸ä¼¼åº¦å¼‚å¸¸é«˜çš„é—®é¢˜
ä¸»è¦è§£å†³ï¼šæƒé‡åˆå§‹åŒ–ã€æ•°å€¼ç¨³å®šæ€§ã€æ¢¯åº¦æµé—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import yaml
import logging
from models.tsadcnn import TSADCNN, TSADCNNConfig
from utils.contrastive_data_loader import create_contrastive_data_loaders

def xavier_init_weights(m):
    """Xavier/Glorotåˆå§‹åŒ–"""
    if isinstance(m, nn.Linear):
        # Xavier uniformåˆå§‹åŒ–
        nn.init.xavier_uniform_(m.weight, gain=1.0)
        if m.bias is not None:
            # å°çš„éé›¶åç½®ï¼Œé¿å…å¯¹ç§°æ€§
            nn.init.uniform_(m.bias, -0.01, 0.01)
    elif isinstance(m, nn.BatchNorm1d):
        # BatchNormå‚æ•°åˆå§‹åŒ–
        nn.init.constant_(m.weight, 1.0)
        nn.init.constant_(m.bias, 0.0)
    elif isinstance(m, nn.Conv2d):
        # å·ç§¯å±‚ä½¿ç”¨Heåˆå§‹åŒ–
        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            nn.init.uniform_(m.bias, -0.01, 0.01)

def he_init_weights(m):
    """He/Kaimingåˆå§‹åŒ–ï¼ˆé€‚åˆReLUæ¿€æ´»ï¼‰"""
    if isinstance(m, nn.Linear):
        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            nn.init.uniform_(m.bias, -0.01, 0.01)
    elif isinstance(m, nn.BatchNorm1d):
        nn.init.constant_(m.weight, 1.0)
        nn.init.constant_(m.bias, 0.0)
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            nn.init.uniform_(m.bias, -0.01, 0.01)

def orthogonal_init_weights(m):
    """æ­£äº¤åˆå§‹åŒ–ï¼ˆä¿æŒæ¢¯åº¦æµï¼‰"""
    if isinstance(m, nn.Linear):
        nn.init.orthogonal_(m.weight, gain=1.0)
        if m.bias is not None:
            nn.init.uniform_(m.bias, -0.01, 0.01)
    elif isinstance(m, nn.BatchNorm1d):
        nn.init.constant_(m.weight, 1.0)
        nn.init.constant_(m.bias, 0.0)
    elif isinstance(m, nn.Conv2d):
        nn.init.orthogonal_(m.weight, gain=1.0)
        if m.bias is not None:
            nn.init.uniform_(m.bias, -0.01, 0.01)

def add_noise_regularization(embeddings, noise_std=0.01):
    """æ·»åŠ å™ªå£°æ­£åˆ™åŒ–ï¼Œå¢åŠ åµŒå…¥å¤šæ ·æ€§"""
    if embeddings.requires_grad:
        noise = torch.randn_like(embeddings) * noise_std
        return embeddings + noise
    return embeddings

def stable_cosine_similarity(x, y, eps=1e-8):
    """æ•°å€¼ç¨³å®šçš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""
    # ç¡®ä¿è¾“å…¥å·²å½’ä¸€åŒ–
    x_norm = F.normalize(x, p=2, dim=1, eps=eps)
    y_norm = F.normalize(y, p=2, dim=1, eps=eps)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé™åˆ¶èŒƒå›´
    cos_sim = torch.sum(x_norm * y_norm, dim=1)
    cos_sim = torch.clamp(cos_sim, -1.0 + eps, 1.0 - eps)
    
    return cos_sim

def stable_euclidean_distance(cos_sim, eps=1e-8):
    """æ•°å€¼ç¨³å®šçš„æ¬§å‡ é‡Œå¾—è·ç¦»è®¡ç®—"""
    # ç¡®ä¿cos_simåœ¨æœ‰æ•ˆèŒƒå›´å†…
    cos_sim = torch.clamp(cos_sim, -1.0 + eps, 1.0 - eps)
    
    # ä½¿ç”¨ç¨³å®šçš„å…¬å¼ï¼šd = sqrt(2 * (1 - cos_sim))
    distance_squared = 2.0 * (1.0 - cos_sim)
    distance_squared = torch.clamp(distance_squared, eps, 4.0)  # é™åˆ¶åœ¨[eps, 4]
    
    return torch.sqrt(distance_squared)

def improved_contrastive_loss(z_old, z_new, labels, margin=0.5, pos_weight=1.0, neg_weight=1.0, eps=1e-8):
    """æ”¹è¿›çš„å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œå¢å¼ºæ•°å€¼ç¨³å®šæ€§"""
    # æ·»åŠ å™ªå£°æ­£åˆ™åŒ–
    z_old = add_noise_regularization(z_old, noise_std=0.01)
    z_new = add_noise_regularization(z_new, noise_std=0.01)
    
    # ç¨³å®šçš„ç›¸ä¼¼åº¦è®¡ç®—
    cos_sim = stable_cosine_similarity(z_old, z_new, eps=eps)
    euclidean_dist = stable_euclidean_distance(cos_sim, eps=eps)
    
    # å¯¹æ¯”æŸå¤±è®¡ç®—
    pos_loss = labels * torch.pow(euclidean_dist, 2)
    neg_loss = (1 - labels) * torch.pow(torch.clamp(margin - euclidean_dist, min=0.0), 2)
    
    # åŠ æƒæŸå¤±
    total_loss = pos_weight * pos_loss + neg_weight * neg_loss
    
    return total_loss.mean(), {
        'cos_sim_mean': cos_sim.mean().item(),
        'cos_sim_std': cos_sim.std().item(),
        'euclidean_dist_mean': euclidean_dist.mean().item(),
        'euclidean_dist_std': euclidean_dist.std().item(),
        'pos_loss_mean': pos_loss.mean().item(),
        'neg_loss_mean': neg_loss.mean().item()
    }

def test_initialization_methods():
    """æµ‹è¯•ä¸åŒåˆå§‹åŒ–æ–¹æ³•çš„æ•ˆæœ"""
    print("ğŸ”§ æµ‹è¯•ä¸åŒæƒé‡åˆå§‹åŒ–æ–¹æ³•...")
    
    # åŠ è½½é…ç½®
    with open("config_improved_v4.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = TSADCNNConfig(
        input_dim=config['model']['input_dim'],
        encoder_hidden_dim=config['model']['encoder_hidden_dim'],
        encoder_output_dim=config['model']['encoder_output_dim'],
        projection_hidden_dim=config['model']['projection_hidden_dim'],
        projection_output_dim=config['model']['projection_output_dim'],
        encoder_layers=config['model']['encoder_layers'],
        projection_layers=config['model']['projection_layers'],
        dropout=config['model']['dropout'],
        sequence_length=config['data']['sequence_length'],
        share_backbone=True,
        pos_weight=1.0,
        neg_weight=1.0,
        lambda_symmetric=config['loss']['lambda_sym'],
        margin=config['loss']['margin']
    )
    
    # åŠ è½½æ•°æ®
    try:
        train_loader, test_loader = create_contrastive_data_loaders(
            train_path="data/train_correct.npy",
            test_path="data/test_correct.npy",
            batch_size=config['data']['batch_size'],
            num_workers=0
        )
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•ä¸åŒåˆå§‹åŒ–æ–¹æ³•
    init_methods = {
        "é»˜è®¤åˆå§‹åŒ–": None,
        "Xavieråˆå§‹åŒ–": xavier_init_weights,
        "Heåˆå§‹åŒ–": he_init_weights,
        "æ­£äº¤åˆå§‹åŒ–": orthogonal_init_weights
    }
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    for method_name, init_func in init_methods.items():
        print(f"\nğŸ“Š æµ‹è¯• {method_name}:")
        
        # åˆ›å»ºæ¨¡å‹
        model = TSADCNN(model_config).to(device)
        
        # åº”ç”¨åˆå§‹åŒ–
        if init_func is not None:
            model.apply(init_func)
        
        model.eval()
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        try:
            batch = next(iter(train_loader))
            old_traj, new_traj, labels = batch
            old_traj = old_traj.to(device)
            new_traj = new_traj.to(device)
            labels = labels.to(device)
            
            with torch.no_grad():
                # è·å–åµŒå…¥
                old_emb, _ = model.encode_trajectory(old_traj)
                new_emb, _ = model.encode_trajectory(new_traj)
                
                # è®¡ç®—ç›¸ä¼¼åº¦ç»Ÿè®¡
                cos_sim = stable_cosine_similarity(old_emb, new_emb)
                euclidean_dist = stable_euclidean_distance(cos_sim)
                
                # æ”¹è¿›çš„æŸå¤±è®¡ç®—
                loss, loss_stats = improved_contrastive_loss(
                    old_emb, new_emb, labels, 
                    margin=model_config.margin
                )
                
                print(f"  - ä½™å¼¦ç›¸ä¼¼åº¦: å‡å€¼={cos_sim.mean():.4f}, æ ‡å‡†å·®={cos_sim.std():.4f}")
                print(f"  - æ¬§å‡ é‡Œå¾—è·ç¦»: å‡å€¼={euclidean_dist.mean():.4f}, æ ‡å‡†å·®={euclidean_dist.std():.4f}")
                print(f"  - å¯¹æ¯”æŸå¤±: {loss:.6f}")
                print(f"  - åµŒå…¥èŒƒæ•°: å‡å€¼={old_emb.norm(dim=1).mean():.4f}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰NaN
                if torch.isnan(loss):
                    print(f"  âŒ æŸå¤±ä¸ºNaN")
                else:
                    print(f"  âœ… æŸå¤±æ­£å¸¸")
                
                # åˆ†ææ­£è´Ÿæ ·æœ¬åˆ†ç¦»åº¦
                pos_mask = labels == 1
                neg_mask = labels == 0
                
                if pos_mask.sum() > 0 and neg_mask.sum() > 0:
                    pos_sim = cos_sim[pos_mask].mean()
                    neg_sim = cos_sim[neg_mask].mean()
                    separation = pos_sim - neg_sim
                    print(f"  - æ­£æ ·æœ¬ç›¸ä¼¼åº¦: {pos_sim:.4f}")
                    print(f"  - è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦: {neg_sim:.4f}")
                    print(f"  - åˆ†ç¦»åº¦: {separation:.4f}")
                
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_initialization_methods()